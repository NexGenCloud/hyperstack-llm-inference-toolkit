git:
  token: null
  url: 'https://github.com/NexGenCloud/hyperstack-llm-inference-toolkit'
  branch: main
env:
  APP_PASSWORD: 'APP_PASS'
  SECRET_KEY: 'f8btJXskUwiPQTk5dVpSf1Nkiz5Q5RRe'
  ADMIN_API_KEY: '52ce146e-c505-4bd6-b541-4b0ea8d6854c'
  SQLALCHEMY_DATABASE_URI: 'mysql+pymysql://devuser:devpass@db:3306/inference_db'
  MYSQL_ROOT_PASSWORD: password
  MYSQL_USER: devuser
  MYSQL_PASSWORD: devpass
  MYSQL_DATABASE: inference_db
  MYSQL_DB_HOST: db
  CELERY_BROKER_URL: 'redis://redis:6379'
  CELERY_RESULT_BACKEND: 'redis://redis:6379'
  HYPERSTACK_API_KEY: null
  S3_BUCKET_NAME: null
  S3_ENDPOINT_URL: null
  S3_ACCESS_KEY: null
  S3_SECRET_KEY: null
  DB_BACKUP_SCHEDULE_MIN: '0'
  DB_BACKUP_SCHEDULE_HOUR: '*/6'
  DB_BACKUP_SCHEDULE_DAY_OF_WEEK: '*'
  DB_BACKUP_SCHEDULE_DAY_OF_MONTH: '*'
  DB_BACKUP_SCHEDULE_MONTH_OF_YEAR: '*'
hyperstack_api_key: null
proxy_instance:
  name: 'proxy-vm'
  environment_name: 'default-CANADA-1'
  image_name: 'Ubuntu Server 22.04 LTS R535 CUDA 12.2'
  flavor_name: 'n1-cpu-medium'
  key_name: 'default-CANADA-1-key'
  security_rules:
    - direction: 'ingress'
      protocol: 'tcp'
      ethertype: 'IPv4'
      remote_ip_prefix: '0.0.0.0/0'
      port_range_min: 5001
      port_range_max: 5001
    - direction: 'ingress'
      protocol: 'tcp'
      ethertype: 'IPv4'
      remote_ip_prefix: '0.0.0.0/0'
      port_range_min: 8501
      port_range_max: 8501
    - direction: 'ingress'
      protocol: 'tcp'
      ethertype: 'IPv4'
      remote_ip_prefix: '0.0.0.0/0'
      port_range_min: 22
      port_range_max: 22
  # user data will be added by us during runtime to deploy the proxy api
inference_engine_vms:
  - name: "inference-vm-1"
    environment_name: "default-CANADA-1"
    flavor_name: "n1-RTX-A6000x1"
    image_name: "Ubuntu Server 22.04 LTS R535 CUDA 12.2"
    port: 8000
    assign_floating_ip: "True"
    model_name: "NousResearch/Meta-Llama-3.1-8B-Instruct"
    key_name: "canada-key-prod-040624"
    run_command: |
      # You might need /ephemeral/ if using a large model
      mkdir -p /home/ubuntu/data/hf
      docker run -d --gpus all \
      -v /home/ubuntu/data/hf:/root/.cache/huggingface \
      -p 8000:8000 \
      --ipc=host --restart always \
      vllm/vllm-openai:latest --model "NousResearch/Meta-Llama-3.1-8B-Instruct" \
      --gpu-memory-utilization 0.9 --max-model-len 15360 --chat-template examples/tool_chat_template_llama3.1_json.jinja


  - name: 'inference-vm-2'
    environment_name: 'default-CANADA-1'
    flavor_name: 'n1-RTX-A6000x1'
    key_name: 'default-CANADA-1-key'
    image_name: 'Ubuntu Server 22.04 LTS R535 CUDA 12.2'
    assign_floating_ip: 'True'
    model_name: 'microsoft/Phi-3-medium-128k-instruct'
    run_command: |
      mkdir /home/ubuntu/data/hf
      docker run -d --gpus all \
      -v /home/ubuntu/data/hf:/root/.cache/huggingface \
      -p 8000:8000 \
      --ipc=host --restart always \
      vllm/vllm-openai:latest --model "microsoft/Phi-3-medium-128k-instruct" \
      --gpu-memory-utilization 0.9 --max-model-len 15360
    port: 8000
